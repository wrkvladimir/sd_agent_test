# SD Agent Test — прототип чат‑агента поддержки

Этот репозиторий реализует прототип AI‑агента поддержки с RAG‑поиском, слоем инструментов (Tools / Поведенческих сценарев) и простым web‑UI для тестирования всех методов.

---

## Допущения и ограничения прототипа

- **Precondition сценариев.** В текущей версии все сценарии считаются применимыми только к **первому сообщению** диалога (`message_index == 1`). Отдельного управляемого поля `preconditions` в JSON‑описании сценария нет, но предполагается, что в будущем предварительными условиявми можно управлять на этапе загрузки.
- **Нет «умных» LLM‑precondition‑запросов.** Мы не делаем дополнительный LLM‑вызов, чтобы решать, подключать ли сценарий. Вместо этого для сценария «День рождения» используется дешёвый текстовый **префильтр по ключевым словам**.
- **Нет слоя Guardrails.** Никакие внешние guardrails / модерация контента не подключались — фокус на архитектуре RAG + Tools.
- **Одна и та же модель для ответов и summary.** Для саммари используется тот же LLM, что и для основного диалога; отдельную «дешёвую» summarization‑модель не поднимал.
- **LLM‑провайдер.** Используется бесплатная open‑source модель `tngtech/deepseek-r1t2-chimera:free` через OpenRouter (OpenAI‑совместимый API).  

---

## Архитектура решения

Проект разбит на несколько сервисов (все поднимаются через `docker-compose`):

- **`ingest-and-retrieval`** — FastAPI‑сервис для инжеста HTML и поиска по базе знаний (RAG‑слой).
  - Читает HTML, режет на чанки, считает эмбеддинги, пишет в Qdrant.
  - Выполняет векторный поиск + rerank (SentenceTransformers).
- **`chat-app`** — основной чат‑агент.
  - Хранит состояние диалога в Redis (с фоллбеком на in‑memory).
  - Делает запросы в `ingest-and-retrieval` за чанками.
  - Прогоняет **Tools‑слой** 
  - Как частность Tool - возможность дополнять контекст по сценарию (в т.ч. сценарий «День рождения»).
  - Собирает YAML‑подобный промпт и вызывает LLM через OpenRouter.
  - Асинхронно обновляет краткое summary диалога.
- **`ui-backend`** + статический фронт (`ui_backend/static/index.html`)
  - Прокси над обоими API, реализует long‑running chat jobs и polling.
  - Отдаёт минималистичный, но удобный UI на `http://localhost:8080`.
- Инфраструктура:
  - **Qdrant** — векторное хранилище.
  - **Redis** — память диалогов.

---

## Сквозной пайплайн обработки сообщения

Ниже — как обрабатывается каждый запрос `POST /chat` в `chat-app`:

1. **Загрузка и обновление состояния диалога**
   - `ChatOrchestrator.handle_chat` получает `conversation_id`.
   - Через `BaseConversationMemory` (Redis / in‑memory) загружается `ConversationState`.
   - Инкрементируется `message_index`.
   - Для **первого сообщения** вызывается tool‑стаб `get_user_data`:
     - генерируется случайное русское имя и возраст (18–120),
     - эти данные сохраняются в `state.user_profile`.
   - Сообщение пользователя (`message`) добавляется в историю (`HistoryItem(role=user)`).

2. **Retrieval (R): поиск по базе знаний**
   - `KBRetriever` делает `POST` в сервис `ingest-and-retrieval` на `/search`.
   - Там:
     - текст запроса кодируется через BAAI/bge-m3;
     - Qdrant ищет ближайшие точки;
     - результаты фильтруются по порогу `SEARCH_SCORE_THRESHOLD`;
     - при включённом `RERANKER_ENABLED` используется reranker BAAI/bge-reranker-v2-m3;
   - Агент получает `top_k` чанков (структура `Chunk` с `text` и `metadata`).

3. **Tools**
   - Для каждого зарегистрированного сценария из `scenario_registry` вызывается `ScenarioToolRunner.run(...)`.
   - **Precondition для всех сценариев:** `state.message_index == 1` (только первое сообщение диалога).
   - Дополнительно для сценария «День рождения» применяется **простой префильтр по триггерам**  
     (`"день рождения"`, `"днюха"`, `"др"`, «сегодня день рождения», «день рождения в августе» и т.п.).
     - Если триггеров в первом сообщении нет, сценарий вообще не попадает в промпт.

   **Внутри `ScenarioToolRunner` сценарий интерпретируется как код:**

   - JSON `ScenarioDefinition` (`data/test_scenario.json`) содержит список нод `code`:
     - `type: "text"` — текстовые инструкции;
     - `type: "tool"` — вызов инструмента (например, `"get_user_data"`);
     - `type: "if"` с `condition`, `children`, `else_children`;
     - `type: "end"` — завершение сценария.
   - Ноды сортируются по `id` (натуральная сортировка: `3` < `3.1` < `4`).
   - Поддерживается **шаблонизация** в текстах (`{=...=}`):
     - `{=@get_user_data.name=}` — данные, возвращённые tool'ом;
     - `{=dialog.name=}`, `{=dialog.age=}`, `{=dialog.message_index=}` — из `ConversationState`.

   **Результат сценария** — YAML‑объект `special_instructions`

   Логика `when_true` / `when_false` внутри `instructions` описана максимально явно:

   - Сначала модель решает, касается ли `user_message` вообще темы из `description`.
     - Если **не касается** — блок полностью игнорируется (ни `when_true`, ни `when_false`).
   - Если касается:
     - `TRUE` — когда из сообщения явно следует, что условие выполняется  
       (например: «сегодня у меня день рождения»).
     - `FALSE` — когда явно сказано, что условие **не** выполняется, но тема та же  
       («день рождения был на прошлой неделе», «в августе» и т.п.).
   - Если однозначно решить нельзя — блок лучше полностью игнорировать.
   - В результате `special_instructions` аккуратно дополняют контекст, но не перетягивают на себя логику, когда пользователь вообще не говорит про ДР.

4. **Generation (G): сборка промпта и вызов LLM**

   - `PromptBuilder.build_prompt(...)` собирает **один YAML‑подобный system‑промпт**, в котором лежит всё:

     - `system:` — главные правила
     - `assistant_meta:` — памятка: как отвечать на «Кто ты?» и «О чем мы общаемся?».
     - `dialog_params:` — текущее состояние диалога и мета-параметры:
     - `dialog_summary:` — краткое резюме (обновляется Summarizer'ом).
     - `dialog_tail:` — несколько последних сообщений (user/assistant) в компактном формате.
     - `context:` — top‑k чанков из базы знаний:
     - `special_instructions:` — YAML из сценариев/инструментов (см. выше).
     - `new_user_message:` — последнее сообщение пользователя.

   - `LLMClient` вызывает OpenRouter (`OPENAI_BASE_URL`, `OPENAI_API_KEY`, `LLM_MODEL`) и возвращает текст ответа. Все payload'ы логируются в JSON, чтобы можно было воспроизвести запрос через `curl`.

5. **Сохранение ответа и обновление summary**

   - Ответ ассистента добавляется в историю и сохраняется в Redis.
   - Параллельно через `BackgroundTasks` запускается `Summarizer.update_summary(...)`
   - LLM получает инструкцию «Вы спрашивали..., я объяснил...» и отдаёт короткое повествовательное резюме.
   - Результат кладётся в `state.summary`.

---

## Методы приложения

### 1. HTTP API

- `POST /chat` (сервис `chat-app`)
  - Request:
    - `conversation_id: string`
    - `message: string`
  - Response:
    - `conversation_id: string`
    - `answer: string` — итоговый ответ агента.
    - `chunks: list[Chunk]` — использованные чанки RAG.
    - `last_step_scenario: Optional[str]` — имя последнего сработавшего сценария (сейчас — название сценария ДР).

Дополнительно реализованы:

- `GET /history?conversation_id=...` — вся история диалога.
- `GET /summary?conversation_id=...` — текущее краткое summary (обновляется в фоне).
- Управление сценариями:
  - `POST /scenarios` — добавить/обновить сценарий.
  - `GET /scenarios` — список сценариев.
  - `DELETE /scenarios/{name}` — удалить сценарий.

Swagger UI для `chat-app`:  
**http://localhost:8000/docs**

### 2. База знаний (RAG)

#### 2.1 Ingestion / индексирование

Реализовано отдельным сервисом `ingest-and-retrieval`:

- Векторная БД: **Qdrant**, поднимается через `docker-compose`.
- Инжест HTML:
  - `POST /ingest` (`ingest_and_retrieval.main.ingest`)
  - Читает HTML либо из файла (`source_type=file`, `path=data/test.html`), либо из поля `html` (`source_type=inline_html`).
  - Режет на чанки, считает эмбеддинги, загружает в Qdrant.
  - Параметры:
    - `CHUNK_MAX_LENGTH`, `CHUNK_OVERLAP`
    - `SEARCH_TOP_K`, `SEARCH_LIMIT`, `SEARCH_SCORE_THRESHOLD`
  - Метаданные каждого чанка включают:
    - `title`, `source_id`, `source_date`, `source_label`, `source_document_id`, `vector_score`.

Отдельный режим/скрипт не нужен — инжест выполняется через HTTP‑метод + UI:

- по умолчанию в `docker-compose up` коллекция уже может быть наполнена `data/test.html`;
- либо в UI можно выбрать любой HTML‑файл и загрузить его.

Вспомогательные методы:

- `GET /data` — посмотреть все точки коллекции.
- `POST /clear_data` — очистить коллекцию.

Swagger UI для `ingest-and-retrieval`:  
**http://localhost:8001/docs**

#### 2.2 Retrieval + ответ

На каждом `POST /chat`:

- поднимается история/summary по `conversation_id` из Redis;
- формируется запрос к `/search` в ingest‑сервис;
- берутся `top_k` чанков (после порога и, опционально, rerank'а);
- ответ LLM строится строго на основе:
  - текста этих чанков (`context`),
  - текстовых блоков сценария (`special_instructions.blocks` / `blocks_with_conditions`);
- если релевантных чанков нет — в system‑промпте заложен fallback:
  - агент честно пишет, что не нашёл точной информации, и предлагает эскалацию/переформулировку.

### 3. Память диалога, history и summary

- **Память**: `RedisConversationMemory` (`chat_app/memory.py`).
  - Ключи:
    - `conv:{conversation_id}:state` — сериализованный `ConversationState`.
    - `conv:{conversation_id}:history` — список `HistoryItem` (JSON).
  - Есть `InMemoryConversationMemory` как фоллбек.
- **Summary**:
  - после каждого ответа запускается `Summarizer.update_summary` (асинхронно);
  - summary хранится в `ConversationState.summary` и попадает в промпт (`dialog_summary`) и в `/summary`.

### 4. Сценарий «Начисление бонусов ко дню рождения»

Сценарий лежит в `data/test_scenario.json` и автоматически загружается при старте `chat-app`, хардкода нет: сценарий разбирается гибко по каждому id.

- **Tool `get_user_data`**
  - реализован как функция‑стаб (`chat_app/tools/user_data.py`);
  - на первом сообщении диалога:
    - генерирует случайное русское имя и возраст;
    - имя используется в обращении и в сценарии.

- **Понимание «сегодня день рождения или нет»**
  - Для сценария про ДР:
    - общий precondition: `message_index == 1`;
    - дополнительный текстовый префильтр по ключевым словам в первом сообщении.
  - Внутри LLM:
    - `blocks_with_conditions` содержат `description` и `user_message`;
    - в `instructions` жёстко описано, как отличать:
      - «сегодня день рождения» (ветка `when_true`);
      - «день рождения не сегодня / в прошлом / в будущем» (ветка `when_false`);
      - сообщения, вообще не про ДР (игнорировать блок).

- **Ответ сценария:**
  - Ветвление по birthday‑условию +
  - обращение по имени из `get_user_data` +
  - параллельно ответ на основной вопрос из базы знаний (RAG, `context`).

---

## Как запустить проект

### 1. Подготовить `.env`

Скопируйте `.env.example` в `.env` и заполните как минимум:

```bash
cp .env.example .env
```

В `.env` задайте:

- `OPENAI_API_KEY=sk-or-v1-...` — ваш ключ OpenRouter;
- `OPENAI_BASE_URL=https://openrouter.ai/api/v1`;
- `LLM_MODEL=tngtech/deepseek-r1t2-chimera:free`.

Остальные параметры можно оставить по умолчанию.

### 2. Поднять окружение

```bash
docker compose up --build
```

Будут подняты:

- Qdrant (`localhost:6333`),
- Redis (`localhost:6379`),
- ingest‑service (`localhost:8001`),
- chat‑app (`localhost:8000`),
- UI‑backend + фронт (`localhost:8080`).

Первый запуск может занимать продолжительное время.
---

## Как работать через UI

Откройте в браузере:  
**http://localhost:8080/**

### Левая колонка — управление знаниями

- **Choose File** — выберите HTML‑файл (например, другой FAQ).
- **Загрузить инжест**:
  - отправляет файл как `inline_html` на `/api/ingest` → `ingest-and-retrieval /ingest`;
  - результат (сколько чанков проиндексировано, предупреждения) отображается в блоке «Ответ сервиса данных».
- **Очистить данные** — очистка коллекции Qdrant через `/api/clear` → `/clear_data`.
- **Получить содержимое базы** — прокси на `/data`, показывает список всех точек.

### Две чат‑панели

Каждая панель полностью независима:

- Поле `conversation_id`:
  - можно ввести руками;
  - можно нажать **New**, чтобы сгенерировать `conv-<uuid>`.
- Выпадающий список **Диалоги...**:
  - подгружается из Redis (`/api/chat/conversations`);
  - позволяет вернуться к любой ранее начатой сессии.
- Кнопки:
  - **History** — подгрузить историю выбранного диалога (`/api/chat/history` → `/history`).
  - **Summary** — получить краткое summary (`/api/chat/summary` → `/summary`) и показать его в отдельном блоке под историей.
- Нижняя часть:
  - поле ввода сообщения + кнопка **Send**;
  - отправка через `/api/chat/send` создаёт long‑running задачу;
  - UI показывает «Агент печатает...» и опрашивает `/api/chat/poll` до готовности результата.

Таким образом удобно тестировать:

- два диалога с разными `conversation_id` параллельно;
- сценарий «День рождения» в одном чате и обычный RAG в другом;
- работу summary по мере роста истории.

---

## Как тестировать через API (curl)

### 1. Базовый чат без дня рождения

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "conversation_id": "conv-1",
    "message": "Привет! Как вывести деньги из копилки?"
  }'
```

Ответ содержит:

- `answer` — нормальный человеко‑понятный ответ;
- `chunks` — список использованных чанков;
- `last_step_scenario` — `null`, т.к. сценарий не сработал.

### 2. Сценарий ДР: день рождения сегодня

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "conversation_id": "bday-1",
    "message": "Привет! Как вывести деньги из копилки? У меня сегодня день рождения."
  }'
```

Ожидаемое поведение:

- обращение по имени (случайно сгенерированному);
- явное поздравление и фраза про начисление **500 бонусов**;
- корректный ответ по базе знаний;
- `last_step_scenario` = `"Начисление бонусов для пользователя, к Дню рождения."`.

### 3. Сценарий ДР: день рождения не сегодня

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "conversation_id": "bday-2",
    "message": "Привет! Как вывести деньги из копилки? У меня день рождения был на прошлой неделе."
  }'
```

Ожидаемое поведение:

- обращение по имени;
- объяснение, что бонусы начисляются **только в сам день рождения**;
- ответ по базе знаний.

### 4. Разделение контекста по conversation_id

Два независимых диалога:

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"conversation_id": "conv-A", "message": "Привет! Как вывести деньги из копилки?"}'

curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"conversation_id": "conv-B", "message": "Привет! Кто ты?"}'
```

- История и summary для `conv-A` и `conv-B` не пересекаются.
- В `conv-B` на вопрос «Кто ты?» агент использует `assistant_meta` и отвечает, что он виртуальный агент поддержки.

### 5. История и summary по API

```bash
curl "http://localhost:8000/history?conversation_id=conv-1"

curl "http://localhost:8000/summary?conversation_id=conv-1"
```

- `/history` возвращает последовательность `user` / `assistant` с таймстампами.
- `/summary` — краткое повествовательное резюме вида:  
  `Вы спрашивали о ..., я объяснил, что ...`.

---