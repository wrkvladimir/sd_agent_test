## **Тестовое задание**

### **Контекст**

Клиент пишет в чат поддержки вопрос по продукту. В API запросе всегда есть conversation\_id, по нему агент понимает: это продолжение диалога или новый чат.

Агент должен:

* поднимать историю/summary по conversation\_id — реализацию этого механизма выбираете вы; допускается in-memory память внутри процесса (без Redis/БД). В этом случае история сохраняется только пока запущен сервис; при перезапуске приложения память очищается и диалоги начинаются заново;

* искать ответ в базе знаний (FAQ/документация) через векторный поиск;

* формировать “человеческий” ответ **только** на основе найденных фактов;

* если подходящего ответа нет — честно сообщать и предлагать переформулировать запрос или эскалировать его живому специалисту;

* уметь “прогонять” упрощённый сценарий “Начисление бонусов ко дню рождения”.

Ваша задача — сделать прототип AI-агента для технической поддержки, который:

* отвечает на основе контекста и может помочь с решением проблемы;

* может выполнять прототип сценария “День рождения”;

* если он чего-то не знает или у него недостаточно знаний — зовёт специалиста;

* понимает, что он агент поддержки (проверяется вопросом в чат: **«Кто ты?»**).  
* владеет историей сообщений \- диалога. (проверяется вопросом в чат: **«О чем мы общаемся?»**)

---

## **1\) Требования к функциональности**

### **1.1 HTTP API (минимум)**

**POST /chat**

Request JSON:

* conversation\_id: string (обяз.)

* message: string (обяз.)

Response JSON:

* conversation\_id: string

* answer: string

* chunks: list

* last\_step\_scenario: str

---

## **2\) База знаний (RAG)**

### **2.1 Ingestion / индексирование**

Векторное хранилище/БД можно выбрать самостоятельно (предпочтительнее Qdrant). Разрешено хранить индекс локально в репозитории либо поднять БД через docker-compose.

Если индекс БД **не хранится локально в репозитории** (то есть документы не проиндексированы заранее), то в проекте **обязательно** должен быть скрипт/сценарий, запуск которого:

* проиндексирует файл с контекстом,

* создаст вектора/эмбеддинги,

* загрузит данные в векторную БД,

   чтобы можно было проверить решение.

Нужно сделать отдельный режим/скрипт/команду, который:

* читает входной документ(ы) FAQ/документации,

* режет на чанки,

* считает эмбеддинги,

* загружает в векторную БД (Qdrant или аналог).

В индекс нужно сохранить:

* chunk\_id

* text

* metadata: dict — если посчитаете нужным и какую посчитаете нужным

Параметры чанкинга и top\_k поиска должны задаваться конфигом.

### **2.2 Retrieval \+ ответ**

На каждый запрос:

* достать history/summary по conversation\_id;

* сформировать поисковый запрос;

* получить top\_k чанков;

* сгенерировать ответ, опираясь **только** на найденные чанки и text-ноды из сценария;

Fallback:

* если релевантных чанков нет (например top\_k пустой или все ниже порога min\_score) — ответить шаблоном:

  * “Не нашёл точной информации в базе знаний…”

  * предложить эскалацию к человеку.

* Также допускается несколько вариантов реализации проверки “агент не знает / недостаточно информации” и политики эскалации (не только по score/min\_score retrieval): например, отдельная **Guardrails-нода** (простой LLM-классификатор или правила), которая анализирует черновик ответа/контекст и, если агент сообщает об отсутствии информации или ответ получается неуверенным/необоснованным, заменяет финальный ответ на шаблонный fallback (“информация не найдена, передали специалисту/предлагаем уточнить”), а также может принудительно отправлять запрос на эскалацию; допустимы и другие политики эскалации (по эвристикам, по результатам проверки цитат, по классификации интента/риска и т.д.).

---

## **3\) Память диалога**

### **3.1 Хранение**

Для каждого conversation\_id хранить:

* список сообщений (хотя бы последние N, например N=20),

* summary (строка) — саммари того, как идет диалог с агентом.

Можно реализовать summary максимально просто:

* либо LLM-summary,

* либо “сжатие” как “ключевые факты \+ последние 3 реплики”.

Главное: summary должен существовать и обновляться.

---

## **4\) Сценарий “Начисление бонусов к дню рождения” (Scenario Runner)**

### **4.1 Что требуется**

Реализовать движок(одна из нод агента, например как retriver-node), который сможет обрабатывать сценарий (пример прикреплен в конце документа).

Движок представляет из себя JSON-структуру, в которой описаны операции:

* text — нода дополнения контекста для LLM. Поддерживает f-строки формата {=variable=}. Подставляет значение из выполненного тула по формату @tools\_name.variable.

* tool — нода, которая вызывает тулу и хранит результат выполнения в in-memory буфере. К результату всегда можно обратиться.

* if — нода, которая выполняет условное ветвление в зависимости от condition. Обработчик пойдет либо в children, либо в else\_children и выполнит операции из ветки. Нода принимает решение выбора с помощью запроса к LLM.

* end — однозначно сообщает, что сценарий завершен.

Сценарий “Начисление бонусов ко дню рождения” должен запускаться **только на первом сообщении в рамках нового conversation\_id** (то есть при первом обращении пользователя в этот диалог): в этом первом ответе агент учитывает логику сценария (вызов get\_user\_data, ветвление по ДР, обращение по имени) **и** одновременно отвечает на вопрос пользователя по базе знаний (RAG). Для всех последующих сообщений с тем же conversation\_id сценарий повторно не запускается — агент продолжает вести диалог в обычном режиме, отвечая только через RAG с учётом накопленной истории/summary.

Агент на ТОЛЬКО ПЕРВОМ запросе пользователя должен учитывать(Запускать) сценарий:

* вызвать get\_user\_data,

* понять, “сегодня у пользователя день рождения” или нет \- согласно запросу пользователя,

* в ответе:

  * всегда обращаться по имени (из get\_user\_data.name),

  * если ДР сегодня — поздравить и сообщить “начислили 500 бонусов”,

  * если ДР не сегодня — вежливо сказать, что бонусы начисляются в сам день рождения,

  * плюс ответить на вопрос пользователя по контексту из базы знаний (RAG).

Важно: не требуется “универсальный движок сценариев”, но требуется понятная архитектура, которая “проходит” конкретный сценарий из JSON.

Пример диалога (1):

* User: Привет\! Как снять деньги с карты?

* AI: Антон, Чтобы снять деньги с карты, обратитесь в ближайшее отделение банка.

* User: Спасибо\!

* AI: Обращайтесь, всегда помогу.

Пример диалога (2):

* User: Привет\! Как снять деньги с карты? У меня кстати сегодня день рождения.

* AI: Виктор, Поздравляем вас, в честь такого события мы начислим вам 500 бонусов. Чтобы снять деньги с карты, обратитесь в ближайшее отделение банка.

* User: Спасибо\!

* AI: Обращайтесь, всегда помогу.

Это лишь пример, который отражает какой результат ожидается.

### **4.2 Tool: get\_user\_data**

Реализовать как функцию-стаб (без настоящих интеграций).

Выход (пример):

{  
  "name": "Антон",  
  "age": "25"  
}  
---

## **8\) Критерии оценки**

* работает end-to-end: ingest → поиск/движок→ ответ → валидация ответа

* корректно различает диалоги по conversation\_id

* промпты хранятся локально либо в коде

* сценарий ДР выполняется (tool \+ if \+ подстановка имени)

* код модульный, конфиги читабельные, без хардкода

---

## **9\) Формат сдачи решения**

* Ссылка на публичный репозиторий (GitHub или аналог).

* В репозитории должны быть:

  * исходный код;

  * README.md с понятной инструкцией запуска и проверки;

  * Dockerfile;

  * docker-compose.yml (минимум: app \+ qdrant, при необходимости redis);

  * .env.example (без секретов).

* В README.md должны быть описаны команды:

  * как поднять окружение (docker compose up ...);

  * как проиндексировать документ/контекст (если индекс не лежит готовым в репозитории);

  * как запустить API;

* Должны быть примеры запросов к API:

  * пример POST /chat (curl/httpie);

  * демонстрация работы минимум с **двумя разными** conversation\_id (контекст не смешивается).

* Индексация:

  * если индекс не включён в репозиторий, обязателен скрипт/команда, которая строит индекс “с нуля” из файла контекста (чанкинг → эмбеддинги → загрузка в векторную БД).

## **Материалы**

Сценарий:

data/test_scenario.json

Контекст для RAG:

test.html
